{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e15a23",
   "metadata": {},
   "source": [
    "# imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b40673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import joblib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03000bc",
   "metadata": {},
   "source": [
    " # dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b180f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def load_data(train_path, test_path, binary=True, features_file=None, top_k=None):\n",
    "    \"\"\"\n",
    "    Carica e preprocessa il dataset NSL-KDD.\n",
    "    \"\"\"\n",
    "    # Nomi delle colonne NSL-KDD\n",
    "    columns = [\n",
    "        \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\n",
    "        \"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
    "        \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n",
    "        \"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "        \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\n",
    "        \"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
    "        \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\n",
    "        \"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "        \"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"\n",
    "    ]\n",
    "\n",
    "    # Caricamento dataset\n",
    "    train = pd.read_csv(train_path, names=columns)\n",
    "    test = pd.read_csv(test_path, names=columns)\n",
    "\n",
    "    # Etichetta binaria anziché multiclass\n",
    "    if binary:\n",
    "        train[\"binary_label\"] = train[\"label\"].apply(lambda x: 0 if x == \"normal\" else 1)\n",
    "        test[\"binary_label\"] = test[\"label\"].apply(lambda x: 0 if x == \"normal\" else 1)\n",
    "        target = \"binary_label\"\n",
    "    else:\n",
    "        target = \"label\"\n",
    "\n",
    "    # Encoding categoriche\n",
    "    encoders = {}\n",
    "    for col in [\"protocol_type\", \"service\", \"flag\"]:\n",
    "        le = LabelEncoder()\n",
    "        train[col] = le.fit_transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Selezione feature più importanti\n",
    "    selected_features = None\n",
    "    if features_file is not None:\n",
    "        feat_df = pd.read_csv(features_file)\n",
    "        if top_k is not None:\n",
    "            selected_features = feat_df.head(top_k)[\"feature\"].tolist()\n",
    "        else:\n",
    "            selected_features = feat_df[\"feature\"].tolist()\n",
    "\n",
    "    drop_cols = [\"label\", \"difficulty\", target]\n",
    "    X_train = train.drop(columns=drop_cols)\n",
    "    X_test = test.drop(columns=drop_cols)\n",
    "\n",
    "    if selected_features:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "\n",
    "    y_train = train[target]\n",
    "    y_test = test[target]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # === Salvataggio encoder e scaler per l'inference ===\n",
    "    REPORTS_DIR = \"../reports\"\n",
    "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "    joblib.dump(encoders, os.path.join(REPORTS_DIR, \"encoders.joblib\"))\n",
    "    joblib.dump(scaler, os.path.join(REPORTS_DIR, \"scaler.joblib\"))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def preprocess_sample(\n",
    "    sample,\n",
    "    encoder_path=\"../reports/encoders.joblib\",\n",
    "    scaler_path=\"../reports/scaler.joblib\",\n",
    "    features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocessa un singolo campione (dict) per l'inference.\n",
    "    \"\"\"\n",
    "    # Carica encoder e scaler\n",
    "    encoders = joblib.load(encoder_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "    df = pd.DataFrame([sample])\n",
    "\n",
    "    # Encoding delle categoriche\n",
    "    for col in [\"protocol_type\", \"service\", \"flag\"]:\n",
    "        if col in df.columns and col in encoders:\n",
    "            df[col] = encoders[col].transform(df[col])\n",
    "\n",
    "    if features is not None:\n",
    "        df = df[features]\n",
    "\n",
    "    X = scaler.transform(df)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fe0a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebfe193",
   "metadata": {},
   "source": [
    " # feature_selection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd47baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15-DW1087\\AppData\\Local\\Temp\\ipykernel_23048\\4248627442.py:45: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=feature_importance.head(20), x=\"importance\", y=\"feature\", palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature importance salvata in reports/feature_importance.csv e reports/feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===  Caricamento dataset ===\n",
    "columns = [\n",
    "    \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\n",
    "    \"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
    "    \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n",
    "    \"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"\n",
    "]\n",
    "\n",
    "train = pd.read_csv(\"../data/nsl-kdd/KDDTrain+.TXT\", names=columns)\n",
    "\n",
    "# ===  Preprocessing ===\n",
    "train[\"binary_label\"] = train[\"label\"].apply(lambda x: 0 if x == \"normal\" else 1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in [\"protocol_type\", \"service\", \"flag\"]:\n",
    "    train[col] = encoder.fit_transform(train[col])\n",
    "\n",
    "X = train.drop(columns=[\"label\", \"difficulty\", \"binary_label\"])\n",
    "y = train[\"binary_label\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Training Random Forest ===\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# === Salvataggio risultati ===\n",
    "feature_importance.to_csv(\"../reports/feature_importance.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(data=feature_importance.head(20), x=\"importance\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature per importanza (Random Forest)\", fontsize=14)\n",
    "plt.xlabel(\"Importanza\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\" Feature importance salvata in reports/feature_importance.csv e reports/feature_importance.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea834ad",
   "metadata": {},
   "source": [
    "# torch_models.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fea014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/torch_models.py\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f91ad7",
   "metadata": {},
   "source": [
    "# torch_train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ddaa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Caricamento dataset...\n",
      "[INFO] Numero di feature usate: 20\n",
      "[EPOCH 1/50] Train Loss: 0.0881 | Val Loss: 1.0183 | Val Acc: 0.7662\n",
      "[INFO] Miglior modello salvato\n",
      "[EPOCH 2/50] Train Loss: 0.0550 | Val Loss: 1.0949 | Val Acc: 0.7784\n",
      "[EPOCH 3/50] Train Loss: 0.0470 | Val Loss: 1.1929 | Val Acc: 0.7764\n",
      "[EPOCH 4/50] Train Loss: 0.0423 | Val Loss: 1.2205 | Val Acc: 0.7868\n",
      "[EPOCH 5/50] Train Loss: 0.0386 | Val Loss: 1.4332 | Val Acc: 0.7896\n",
      "[EPOCH 6/50] Train Loss: 0.0364 | Val Loss: 1.5263 | Val Acc: 0.7804\n",
      "[INFO] Early stopping attivato.\n",
      "[INFO] Training completato.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from dataloader import load_data\n",
    "from torch_models import MLPClassifier\n",
    "\n",
    "# === CONFIG ===\n",
    "TRAIN_PATH = \"../data/nsl-kdd/KDDTrain+.txt\"\n",
    "TEST_PATH = \"../data/nsl-kdd/KDDTest+.txt\"\n",
    "FEATURES_FILE = \"../reports/feature_importance.csv\"\n",
    "TOP_K = 20\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5  # early stopping\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === CARTELLE ===\n",
    "REPORTS_DIR = \"../reports\"\n",
    "RUNS_DIR = \"../runs/ids_experiment\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# === CARICAMENTO DATI ===\n",
    "print(\"[INFO] Caricamento dataset...\")\n",
    "X_train, y_train, X_test, y_test = load_data(\n",
    "    train_path=TRAIN_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    binary=True,\n",
    "    features_file=FEATURES_FILE,\n",
    "    top_k=TOP_K\n",
    ")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"[INFO] Numero di feature usate: {input_dim}\")\n",
    "\n",
    "# Conversione in tensori\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Creazione DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# === MODELLO, LOSS E OPTIMIZER ===\n",
    "model = MLPClassifier(input_dim=input_dim).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TensorBoard\n",
    "writer = SummaryWriter(log_dir=RUNS_DIR)\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # === VALIDAZIONE ===\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f\"[EPOCH {epoch+1}/{EPOCHS}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "            f\"Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # TensorBoard logging\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_accuracy, epoch)\n",
    "\n",
    "    # === SALVATAGGIO MODELLO ===\n",
    "    torch.save(model.state_dict(), os.path.join(REPORTS_DIR, \"model_last.pth\"))\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(REPORTS_DIR, \"model_best.pth\"))\n",
    "        print(\"[INFO] Miglior modello salvato\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"[INFO] Early stopping attivato.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "print(\"[INFO] Training completato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732839d",
   "metadata": {},
   "source": [
    "# inference_torch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c954ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modello caricato correttamente.\n",
      "Predizione: normal\n",
      "[INFO] Modello caricato correttamente.\n",
      "Predizione: normal\n"
     ]
    }
   ],
   "source": [
    "# src/inference_torch.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch_models import MLPClassifier\n",
    "from dataloader import preprocess_sample\n",
    "\n",
    "# Config\n",
    "MODEL_PATH = \"../reports/model_best.pth\"\n",
    "FEATURES_FILE = \"../reports/feature_importance.csv\"\n",
    "TOP_K = 20\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carica le feature selezionate\n",
    "features = pd.read_csv(FEATURES_FILE).head(TOP_K)[\"feature\"].tolist()\n",
    "\n",
    "# Carica modello\n",
    "input_dim = len(features)\n",
    "model = MLPClassifier(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"[INFO] Modello caricato correttamente.\")\n",
    "\n",
    "def predict(sample: dict):\n",
    "    \"\"\"\n",
    "    sample: dizionario con le stesse chiavi del dataset NSL-KDD\n",
    "    Esempio:\n",
    "    {\n",
    "        \"duration\": 0,\n",
    "        \"protocol_type\": \"tcp\",\n",
    "        \"service\": \"http\",\n",
    "        \"flag\": \"SF\",\n",
    "        \"src_bytes\": 181,\n",
    "        \"dst_bytes\": 5450,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Preprocessing (usa lo stesso scaler/encoder salvato)\n",
    "    X = preprocess_sample(sample, features=features)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    label = int(predicted.item())\n",
    "    return \"normal\" if label == 0 else \"attack\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Esempiodi un campione normale\n",
    "    sample_normal = {\n",
    "        \"duration\": 0, \"protocol_type\": \"tcp\", \"service\": \"http\", \"flag\": \"SF\",\n",
    "        \"src_bytes\": 181, \"dst_bytes\": 5450, \"land\": 0, \"wrong_fragment\": 0, \"urgent\": 0,\n",
    "        \"hot\": 0, \"num_failed_logins\": 0, \"logged_in\": 1, \"num_compromised\": 0,\n",
    "        \"root_shell\": 0, \"su_attempted\": 0, \"num_root\": 0, \"num_file_creations\": 0,\n",
    "        \"num_shells\": 0, \"num_access_files\": 0, \"num_outbound_cmds\": 0,\n",
    "        \"is_host_login\": 0, \"is_guest_login\": 0, \"count\": 9, \"srv_count\": 9,\n",
    "        \"serror_rate\": 0.00, \"srv_serror_rate\": 0.00, \"rerror_rate\": 0.00,\n",
    "        \"srv_rerror_rate\": 0.00, \"same_srv_rate\": 1.00, \"diff_srv_rate\": 0.00,\n",
    "        \"srv_diff_host_rate\": 0.00, \"dst_host_count\": 9, \"dst_host_srv_count\": 9,\n",
    "        \"dst_host_same_srv_rate\": 1.00, \"dst_host_diff_srv_rate\": 0.00,\n",
    "        \"dst_host_same_src_port_rate\": 0.11, \"dst_host_srv_diff_host_rate\": 0.00,\n",
    "        \"dst_host_serror_rate\": 0.00, \"dst_host_srv_serror_rate\": 0.00,\n",
    "        \"dst_host_rerror_rate\": 0.00, \"dst_host_srv_rerror_rate\": 0.00\n",
    "    }\n",
    "\n",
    "    print(\"Predizione:\", predict(sample_normal))\n",
    "# src/inference_torch.py\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_models import MLPClassifier\n",
    "from dataloader import preprocess_sample\n",
    "\n",
    "# Config\n",
    "MODEL_PATH = \"../reports/model_best.pth\"\n",
    "FEATURES_FILE = \"../reports/feature_importance.csv\"\n",
    "TOP_K = 20\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carica le top-K feature da usare\n",
    "features = pd.read_csv(FEATURES_FILE).head(TOP_K)[\"feature\"].tolist()\n",
    "\n",
    "# Inizializza modello\n",
    "input_dim = len(features)\n",
    "model = MLPClassifier(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"[INFO] Modello caricato correttamente.\")\n",
    "\n",
    "def predict(sample: dict):\n",
    "    \"\"\"\n",
    "    sample: dizionario con tutte le 41 feature originali del dataset NSL-KDD.\n",
    "    \"\"\"\n",
    "    # Preprocessing (usa encoder/scaler salvati + riduzione top-K feature)\n",
    "    X = preprocess_sample(sample, features=features)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    label = int(predicted.item())\n",
    "    return \"normal\" if label == 0 else \"attack\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # campione con tutte le 41 feature NSL-KDD tranne difficulti e labels\n",
    "    sample_example = {\n",
    "        \"duration\": 0,\n",
    "        \"protocol_type\": \"tcp\",\n",
    "        \"service\": \"http\",\n",
    "        \"flag\": \"SF\",\n",
    "        \"src_bytes\": 181,\n",
    "        \"dst_bytes\": 5450,\n",
    "        \"land\": 0,\n",
    "        \"wrong_fragment\": 0,\n",
    "        \"urgent\": 0,\n",
    "        \"hot\": 0,\n",
    "        \"num_failed_logins\": 0,\n",
    "        \"logged_in\": 1,\n",
    "        \"num_compromised\": 0,\n",
    "        \"root_shell\": 0,\n",
    "        \"su_attempted\": 0,\n",
    "        \"num_root\": 0,\n",
    "        \"num_file_creations\": 0,\n",
    "        \"num_shells\": 0,\n",
    "        \"num_access_files\": 0,\n",
    "        \"num_outbound_cmds\": 0,\n",
    "        \"is_host_login\": 0,\n",
    "        \"is_guest_login\": 0,\n",
    "        \"count\": 9,\n",
    "        \"srv_count\": 9,\n",
    "        \"serror_rate\": 0.00,\n",
    "        \"srv_serror_rate\": 0.00,\n",
    "        \"rerror_rate\": 0.00,\n",
    "        \"srv_rerror_rate\": 0.00,\n",
    "        \"same_srv_rate\": 1.00,\n",
    "        \"diff_srv_rate\": 0.00,\n",
    "        \"srv_diff_host_rate\": 0.00,\n",
    "        \"dst_host_count\": 9,\n",
    "        \"dst_host_srv_count\": 9,\n",
    "        \"dst_host_same_srv_rate\": 1.00,\n",
    "        \"dst_host_diff_srv_rate\": 0.00,\n",
    "        \"dst_host_same_src_port_rate\": 0.11,\n",
    "        \"dst_host_srv_diff_host_rate\": 0.00,\n",
    "        \"dst_host_serror_rate\": 0.00,\n",
    "        \"dst_host_srv_serror_rate\": 0.00,\n",
    "        \"dst_host_rerror_rate\": 0.00,\n",
    "        \"dst_host_srv_rerror_rate\": 0.00\n",
    "    }\n",
    "\n",
    "    print(\"Predizione:\", predict(sample_example))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
